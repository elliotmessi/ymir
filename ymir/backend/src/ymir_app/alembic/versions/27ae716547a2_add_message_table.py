"""add message table

Revision ID: 27ae716547a2
Revises: 85e99d360a52
Create Date: 2023-04-03 14:54:52.016893

"""
import json

from alembic import op
import sqlalchemy as sa
from common_utils.percent_log_util import LogState


# revision identifiers, used by Alembic.
revision = "27ae716547a2"
down_revision = "85e99d360a52"
branch_labels = None
depends_on = None


DOCKER_IMAGE_STATE_TO_RESULT_STATE_MAPPING = {
    LogState.PENDING: 0,
    LogState.DONE: 1,
    LogState.PENDING.ERROR: 2,
}


def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "message",
        sa.Column("id", sa.Integer(), autoincrement=True, nullable=False),
        sa.Column("user_id", sa.Integer(), nullable=False),
        sa.Column("project_id", sa.Integer(), nullable=False),
        sa.Column("state", sa.SmallInteger(), nullable=True),
        sa.Column("task_type", sa.SmallInteger(), nullable=True),
        sa.Column("content", sa.String(length=500), nullable=True),
        sa.Column("dataset_id", sa.Integer(), nullable=True),
        sa.Column("model_id", sa.Integer(), nullable=True),
        sa.Column("prediction_id", sa.Integer(), nullable=True),
        sa.Column("docker_image_id", sa.Integer(), nullable=True),
        sa.Column("is_read", sa.Boolean(), nullable=False),
        sa.Column("is_deleted", sa.Boolean(), nullable=False),
        sa.Column("create_datetime", sa.DateTime(), nullable=False),
        sa.Column("update_datetime", sa.DateTime(), nullable=False),
        sa.PrimaryKeyConstraint("id"),
    )
    with op.batch_alter_table("message", schema=None) as batch_op:
        batch_op.create_index(batch_op.f("ix_message_dataset_id"), ["dataset_id"], unique=False)
        batch_op.create_index(batch_op.f("ix_message_id"), ["id"], unique=False)
        batch_op.create_index(batch_op.f("ix_message_model_id"), ["model_id"], unique=False)
        batch_op.create_index(batch_op.f("ix_message_prediction_id"), ["prediction_id"], unique=False)
        batch_op.create_index(batch_op.f("ix_message_docker_image_id"), ["docker_image_id"], unique=False)
        batch_op.create_index(batch_op.f("ix_message_project_id"), ["project_id"], unique=False)
        batch_op.create_index(batch_op.f("ix_message_state"), ["state"], unique=False)
        batch_op.create_index(batch_op.f("ix_message_task_type"), ["task_type"], unique=False)
        batch_op.create_index(batch_op.f("ix_message_user_id"), ["user_id"], unique=False)

    with op.batch_alter_table("project", schema=None) as batch_op:
        batch_op.add_column(sa.Column("recommended_docker_image_id", sa.Integer(), nullable=True))

    with op.batch_alter_table("task", schema=None) as batch_op:
        batch_op.alter_column("last_message_datetime", existing_type=sa.DATETIME(), nullable=True)

    with op.batch_alter_table("dataset", schema=None) as batch_op:
        batch_op.add_column(sa.Column("analysis", sa.JSON(), nullable=True))
    conn = op.get_bind()
    conn.execute("UPDATE dataset SET analysis = '{}' WHERE analysis IS NULL")
    # fill missing keyword_count for existing dataset records
    datasets = conn.execute("SELECT id, keywords FROM dataset WHERE keyword_count IS NULL")
    for row in datasets:
        keyword_count = len(json.loads(row.keywords)) if row.keywords else 0
        conn.execute("UPDATE dataset SET keyword_count = %s WHERE id = %s", (keyword_count, row.id))

    with op.batch_alter_table("docker_image", schema=None) as batch_op:
        batch_op.add_column(sa.Column("result_state", sa.SmallInteger(), nullable=False, server_default="1"))
        batch_op.add_column(sa.Column("is_official", sa.Boolean(), nullable=True))
        batch_op.add_column(sa.Column("task_id", sa.Integer(), nullable=True))
        batch_op.create_index(batch_op.f("ix_docker_image_result_state"), ["result_state"], unique=False)
        batch_op.create_index(batch_op.f("ix_docker_image_task_id"), ["task_id"], unique=False)
        batch_op.create_index(batch_op.f("ix_docker_image_is_official"), ["is_official"], unique=False)
    docker_images = conn.execute("SELECT id, state FROM docker_image")
    for row in docker_images:
        result_state = DOCKER_IMAGE_STATE_TO_RESULT_STATE_MAPPING[int(row.state)]
        conn.execute("UPDATE docker_image SET result_state = %s WHERE id = %s", (result_state, row.id))

    with op.batch_alter_table("docker_image_config", schema=None) as batch_op:
        batch_op.add_column(sa.Column("object_type", sa.SmallInteger(), nullable=False))
        if conn.engine.name != "sqlite":
            # Not supported by sqlite
            batch_op.drop_constraint("PRIMARY", type_="primary")
            batch_op.create_primary_key("PRIMARY", ["image_id", "object_type", "type"])
        batch_op.drop_constraint("unique_image_type", type_="unique")
        batch_op.create_unique_constraint("unique_image_type", ["image_id", "object_type", "type"])
        batch_op.create_index(batch_op.f("ix_docker_image_config_object_type"), ["object_type"], unique=False)
    # migrate object_type from docker_image table
    conn.execute(
        "UPDATE docker_image_config SET object_type = (SELECT object_type FROM docker_image WHERE docker_image.id = docker_image_config.image_id)"
    )

    # ### end Alembic commands ###


def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    conn = op.get_bind()
    with op.batch_alter_table("docker_image_config", schema=None) as batch_op:
        if conn.engine.name != "sqlite":
            # Not supported by sqlite
            batch_op.drop_constraint("PRIMARY", type_="primary")
            batch_op.create_primary_key("PRIMARY", ["image_id", "type"])
        batch_op.drop_index(batch_op.f("ix_docker_image_config_object_type"))
        batch_op.drop_constraint("unique_image_type", type_="unique")
        batch_op.create_unique_constraint("unique_image_type", ["image_id", "type"])
        batch_op.drop_column("object_type")

    with op.batch_alter_table("docker_image", schema=None) as batch_op:
        batch_op.drop_column("is_official")

    with op.batch_alter_table("dataset", schema=None) as batch_op:
        batch_op.drop_column("analysis")


    with op.batch_alter_table("project", schema=None) as batch_op:
        batch_op.drop_column("recommended_docker_image_id")

    with op.batch_alter_table("task", schema=None) as batch_op:
        batch_op.alter_column("last_message_datetime", existing_type=sa.DATETIME(), nullable=False)

    with op.batch_alter_table("message", schema=None) as batch_op:
        batch_op.drop_index(batch_op.f("ix_message_user_id"))
        batch_op.drop_index(batch_op.f("ix_message_task_type"))
        batch_op.drop_index(batch_op.f("ix_message_state"))
        batch_op.drop_index(batch_op.f("ix_message_project_id"))
        batch_op.drop_index(batch_op.f("ix_message_prediction_id"))
        batch_op.drop_index(batch_op.f("ix_message_model_id"))
        batch_op.drop_index(batch_op.f("ix_message_id"))
        batch_op.drop_index(batch_op.f("ix_message_dataset_id"))

    op.drop_table("message")
    # ### end Alembic commands ###
